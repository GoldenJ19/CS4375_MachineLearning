---
title: "Ensemble Techniques"
author: "Justin Hardy & Benjamin Frenkel"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r echo=FALSE, message=FALSE}
# settings/library imports
set.seed(1234)
library(stringr)
library(randomForest)
library(xgboost)
library(adabag)
library(mltools)
```

# The Data Set

Starting by reading in the data set. The data set we'll use for the assignment consists of data collected by an airline organization, over their customers' submitted satisfaction surveys, as well as relevant information about their flight and demographic.

If you want to see the data set for yourself, you access the raw data [here](Invistico_Airline.csv), or the page where I collected it [online](https://www.kaggle.com/datasets/sjleshrac/airlines-customer-satisfaction).

Please note that this part of the assignment reuses the data set used in the [Classification](Classification.pdf) portion of the assignment.

```{r, echo=TRUE}
# Read data set
CustomerData_raw <- read.csv("Invistico_Airline.csv")
```

We'll also remove 90% of the rows in the data set. We'll do this by removing 90% of the satisfied reviews, and 90% of the dissatisfied reviews, and combining what remains.

We're doing this purely to make the Ensemble Techniques run quicker, as otherwise, they'll take way too long to run.

```{r, echo=TRUE}
# Cut down data set, aiming for equal divide between satisfied/dissatisfied
CustomerData_sat <- CustomerData_raw[CustomerData_raw$satisfaction == "satisfied",]
CustomerData_sat <- CustomerData_sat[sample(1:nrow(CustomerData_sat), 
                                            nrow(CustomerData_sat)*0.1, replace=FALSE),]
CustomerData_dis <- CustomerData_raw[CustomerData_raw$satisfaction == "dissatisfied",]
CustomerData_dis <- CustomerData_dis[sample(1:nrow(CustomerData_dis), 
                                            nrow(CustomerData_dis)*0.1, replace=FALSE),]
CustomerData <- rbind(CustomerData_sat[1:(nrow(CustomerData_sat)),], 
                      CustomerData_dis[1:(nrow(CustomerData_dis)),])
```

# Cleaning Up The Data Set

Cleaning up data set for logistic regression, by converting qualitative columns into factors.

```{r, echo=TRUE}
# Factor columns
CustomerData$satisfaction <- factor(CustomerData$satisfaction) # satisfaction
CustomerData$Gender <- factor(CustomerData$Gender) # gender
CustomerData$Customer.Type <- factor(CustomerData$Customer.Type) # customer type
CustomerData$Type.of.Travel <- factor(CustomerData$Type.of.Travel) # travel type
CustomerData$Class <- factor(CustomerData$Class) # class

# Normalize factor names
levels(CustomerData$satisfaction) <- c("Dissatisfied", "Satisfied")
levels(CustomerData$Customer.Type) <- c("Disloyal", "Loyal")
levels(CustomerData$Type.of.Travel) <- c("Business", "Personal")

# Continue factoring numeric finite columns
for(i in 8:21) {
  CustomerData[,i] <- 
    factor(CustomerData[,i], levels=c(0,1,2,3,4,5)) # out-of-5 ratings
}

# Normalize column names
names(CustomerData) <- gsub("\\.", " ", names(CustomerData))
names(CustomerData) <- str_to_title(names(CustomerData))
names(CustomerData) <- gsub("\\ ", ".", names(CustomerData))

# Remove na rows
CustomerData <- CustomerData[complete.cases(CustomerData),]
```

# Dividing Into Train/Test

Dividing the data set into train/test...

```{r, echo=TRUE}
# reset seed
set.seed(1234)

# train/test division
i <- sample(1:nrow(CustomerData), nrow(CustomerData)*0.8, replace=FALSE)
train <- CustomerData[i,]
test <- CustomerData[-i,]
```

# Data Exploration

Please refer to the [Classification](Classification.pdf) portion of this assignment for exploration of this data.

# Models

## Model Training

### DT Random Forest

```{r, echo=TRUE}
# Start time capture
time_start_rt1 <- Sys.time()

# create model
rf1 <- randomForest(Satisfaction~., data=train, importance=TRUE)
summary(rf1)

# Stop time capture
time_end_rt1 <- Sys.time()
runtime_rt1 <- time_end_rt1-time_start_rt1
```

### DT XGBoost

```{r, echo=TRUE}
# train label/matrix
train_label <- ifelse(train$Satisfaction=="Satisfied", 1, 0)
train_matrix <- data.matrix(train[, -1])

# Start time capture
time_start_xgb1 <- Sys.time()

# create model
xgb1 <- xgboost(data=train_matrix, label=train_label,nrounds=100, 
                objective='binary:logistic', verbose=0)
summary(xgb1)

# Stop time capture
time_end_xgb1 <- Sys.time()
runtime_xgb1 <- time_end_xgb1-time_start_xgb1
```

### DT AdaBoost

```{r, echo=TRUE, message=FALSE}
# reset seed
set.seed(1234)

# Start time capture
time_start_ab1 <- Sys.time()

# create model
ab1 <- boosting(Satisfaction~., data=train, boos=TRUE, 
                   mfinal=40, coeflearn='Breiman')
summary(ab1)

# Stop time capture
time_end_ab1 <- Sys.time()
runtime_ab1 <- time_end_ab1-time_start_ab1
```

## Model Predictions

### DT Random Forest

```{r, echo=FALSE}
#prediction
pred_rf1 <- predict(rf1, newdata=test, type="response")

# stats
table(pred_rf1, test$Satisfaction)
acc_rf1 <- mean(pred_rf1==test$Satisfaction)
mcc_rf1 <- mcc(factor(pred_rf1), test$Satisfaction)
cat(paste("Accuracy: ", acc_rf1), paste("MCC: ", mcc_rf1), paste("Runtime: ", runtime_rt1, "s"), sep='\n')
```

### DT XGBoost

```{r, echo=FALSE}
# test label/matrix
test_label <- ifelse(test$Satisfaction=="Satisfied", 1, 0)
test_matrix <- data.matrix(test[, -1])

# prediction
probs_xgb1 <- predict(xgb1, test_matrix)
pred_xgb1 <- ifelse(probs_xgb1>0.5, 1, 0)

# stats
table(pred_xgb1, test_label)
acc_xgb1 <- mean(pred_xgb1==test_label)
mcc_xgb1 <- mcc(pred_xgb1, test_label)
cat(paste("Accuracy: ", acc_xgb1), paste("MCC: ", mcc_xgb1), paste("Runtime: ", runtime_xgb1, "s"), sep='\n')
```

### DT AdaBoost

```{r, echo=FALSE}
# prediction
pred_ab1 <- predict(ab1, newdata=test, type="response")

# stats
table(pred_ab1$class, test$Satisfaction)
acc_ab1 <- mean(pred_ab1$class==test$Satisfaction)
mcc_ab1 <- mcc(factor(pred_ab1$class), test$Satisfaction)
cat(paste("Accuracy: ", acc_ab1), paste("MCC: ", mcc_ab1), paste("Runtime: ", runtime_ab1, "s"), sep='\n')
```

# Analysis

Looking at each of the above prediction results, we can observe a number of things about each ensemble technique. We'll discuss the results of ensemble individually.

## Random Forest

The random forest ensemble seems to run on this data somewhere between 19-21 seconds. It produced an accuracy of 94.06%, which is a considerably good result. The random forest is strong in that it can discover new trees that outperform the basic decision tree, which simply chooses the strongest predictor first to build from. The MCC score is high in comparison to the results of the Classification portion of this assignment, as well.

## XGBoost

XGBoost ran extremely fast on this data set, taking around 1-2 seconds to complete the algorithm. It produced a notably higher accuracy than the random forest ensemble, of 95.48% accuracy, along with a considerably higher MCC value. This algorithm utilizes the machine's multithreading to generate hundreds of trees, which are then aggregated. It's because of this that the algorithm performs at an extremely fast speed, while achieving greater results.

## AdaBoost

AdaBoost ran for roughly the same time as the random forest ensemble, between 20-23 seconds. It produced a slightly better accuracy of 94.40%, as well as a slightly improved MCC value. The algorithm works by iterating through a select number of times - 40 times, in this case - and increases/decreases weights for training examples through each iteration depending on whether or not the error was large/negligible correspondingly. Afterwards, the learners themselves are given a weights, with accurate learners having high weights. For this reason, we're able to achieve a more accurate model than random forest through the means of these iterations.

## Conclusion

It's clear that XGBoost outperforms both Random Forest and AdaBoost in all metrics. Of course, XGBoost is much more modernized and has the advantage of utilizing the machine's multithreaded processing, so this is to be expected.

Compared with our original dive into decision trees in one of the previous assignments, it's clear that ensemble methods help make the decision tree model more dependable as a basis. Of course, we know from the textbook that in general, these ensemble techniques will perform best with low-bias, high-variance learners, so despite its strength, it's important to understand its weakness lies in the opposite.