---
title: "Regression"
author: "Linus Fackler, Justin Hardy, Fernando Colman, Isabelle Villegas"
output:
  word_document: default
  pdf_document: default
---

This data set contains information of over 10000 different plane rides in India in 2019, containing the price, duration, number of stops, and other information.
The set can be found here: https://www.kaggle.com/datasets/ibrahimelsayed182/plane-ticket-price

### Load the data

```{r}
df <- read.csv("planetickets.csv", header = TRUE)
str(df)
```

### Data cleaning

First, we have to clean the data, since a lot of columns contain strings instead of integers or floats.

We throw out some of the columns that don't provide any useful information, such as "additional info".
```{r}
df <- df[,c(1,3,4,6,8,9,11)]
```


We check how many NA rows there are.
```{r}
sapply(df, function(x) sum(is.na(x)==TRUE))
```
This data set contains no rows with missing values, so we don't have to make any changes.

Now, we check how many unique values the column Total_Stops has, before we change it from a String to an Integer.

```{r}
length(unique(df$Total_Stops))
unique(df$Total_Stops)
sum(df$Total_Stops == "")
```

We see that it has 6 unique values, one of them is just the empty string.
There is only 1 row with this empty value, so we delete this row.

```{r}
df <- df[!(df$Total_Stops == ""),]
unique(df$Total_Stops)
```

We change these to usable integer values and then change the type of the column to numeric.

```{r}
df$Total_Stops[df$Total_Stops == "non-stop"] <- 0
df$Total_Stops[df$Total_Stops == "1 stop"] <- 1
df$Total_Stops[df$Total_Stops == "2 stops"] <- 2
df$Total_Stops[df$Total_Stops == "3 stops"] <- 3
df$Total_Stops[df$Total_Stops == "4 stops"] <- 4

unique(df$Total_Stops)

df <- transform(df, Total_Stops = as.integer(Total_Stops))
str(df)
```

Now, for the column "Duration", we will cut off everything after the 'h', so we only keep the hours of the flight.
```{r}
df$Duration <- gsub("h.*","", df$Duration)
df$Duration[1:10]
```
Then, we change the type also to numeric.

```{r}
df <- transform(df, Duration = as.integer(Duration))
str(df)
```
Upon checking, there is somehow an NA value in our column Duration now.
```{r}
sapply(df, function(x) sum(is.na(x)==TRUE))
```
We will simply delete this row, as 1 row does not matter.
```{r}
df <- df[!(is.na(df$Duration)),]
sapply(df, function(x) sum(is.na(x)==TRUE))
```
We're done with duration now.

We do the same for the departure time. We will cut off the minutes and just keep the hours.

```{r}
df$Dep_Time <- gsub(":.*","", df$Dep_Time)
df$Dep_Time[1:10]
```
Now, we will transform it to an integer value.

```{r}
df <- transform(df, Dep_Time = as.integer(Dep_Time))
str(df)
```


### Train and Test sets

Divide into train and test sets

```{r}
set.seed(1234)
i <- sample(1:nrow(df), round(nrow(df)*0.8), replace=FALSE)
train <- df[i, -9]
test <- df[-i, -9]
```


### Data Exploration of Training Data

First, we look at how flight durations and ticket prices look together in a plot.

```{r}
plot(train$Duration~train$Price, xlab="Ticket Price", ylab="Flight Duration")
abline(lm(train$Duration~train$Price), col="red")
```

This shows us the general trend of longer flights resulting in higher ticket prices.


Now, we will see how 
```{r}
x <- train$Price[(train$Price < 15000) & (train$Total_Stops == 0)]
y <- train$Duration[(train$Duration < 20) & (train$Total_Stops == 0)]
plot(y[1:min(length(x),length(y))]~x[1:min(length(x),length(y))], xlab="Ticket Price", ylab="Flight Duration", main="Flights with 0 stops")
```

```{r}
counts <- table(train$Total_Stops)
barplot(counts)
```

We see that there is no data where the number of stops is 4 in the training portion of the data, which is why we will leave it out in the next plot.

```{r}
colors <- c("#2EFF00", #green for 0 stops
            "#FF0000", #red for 1 stop
            "#FFFB00", #yellow for 2 stops
            "#000CFF" #blue for 3 stops
            )

groups <- factor(train$Total_Stops)
plot(train$Duration~train$Price, xlab="Ticket Price", ylab="Flight Duration", pch = 19, col=colors[groups])
legend("topright", legend=c("0 stops", "1 stop", "2 stops", "3 stops"), pch = 19, col = colors[factor(levels(groups))])
abline(lm(train$Duration~train$Price), col="red")
```

This tells us that most short flights have 0 stops and are less than $10000.
It also tells us that the number of stops doesn't necessarily affect the price, rather just the duration.


# Linear Regression

### Model for predictors Duration + Total_Stops

```{r}
lm1 <- lm(Price ~ Duration + Total_Stops, data=train)
summary(lm1)
```

In this Linear Regression model we see the effect that the duration of the flight and number of stops have on the ticket price.
Our R-squared value is around 0.38, which indicates that the duration and number of stops are not the best predictors for this model.
They don't have as big of an effect on the ticket price as I thought. This can be because there are other factors that affect the flight, like the airline, since some are more luxurious and therefore more expensive, or the amount of days the ticket was purchased prior to the flight. If tickets are bought a couple of days before the flight, they are most likely more expensive than tickets that were purchased ahead in time.
The fact that the p-value is less than 0.5 shows that this model is statistically significant.

```{r}
plot(lm1)
```

Residuals vs Fitted:
This represents the difference between the actual price of the plane ticket and our models prediction.
Our model suggests some form of heteroscedasticity, meaning the variability of our predictions are not equally variable throughout. Some values are closer to the 0 line than others. Most values are following a linear relationship though. So there is a clear relationship between the predictors and the predicted value.

Normal Q-Q:
Most data is in between -2 and 2 standard deviations, just like in a normal distributions. There is a bunch of extremes on the right side, which means it is very hard to predict what the price is going to be based off this model.

Scale-Location:
This plot shows if the residuals are spread equally among our predictions in order to check homoscedasticity.
Since there are clear trends in the plot, there is no equal variance in our residuals.

Residuals vs Leverage:
This plot helps us find influential data points.
We don't have any data points that have large leverage and also high residuals, meaning, there are few data points that have a big impact on the coefficients and the intercept of the model. So, we don't have any data points that we should remove necessarily.


### Evaluate on the test set for predictors Duration and Total_Stops
```{r}
pred1 <- predict(lm1, newdata = test)
cor1 <- cor(pred1, test$Price)
mse1 <- mean((pred1 - test$Price) ^2)
rmse1 <- sqrt(mse1)

print(paste("correlation:", cor1))
```
```{r}
print(paste("mse:", mse1))
```
```{r}
print(paste("rmse:", rmse1))
```

### Model with all predictors

```{r}
lma <- lm(Price ~., data = train)
summary(lma)
```
Compared to our model with just Duration and Total_Stops as predictors, this model shows a higher R-squared value, indicating that there are more factors that influence the price of the plane ticket.
As predicted above, the airline carrier makes a difference, since some tend to me more expensive.

### Evaluate model with all predictors

```{r, warning=FALSE}
preda <- predict(lma, newdata = test)
cora <- cor(preda, test$Price)
msea <- mean((preda - test$Price) ^2)
print(paste("cor=", cora))
```
```{r}
print(paste("mse=", msea))
```
We see pretty decent results.
Let's see how they compare to kNN.

# kNN for regression

```{r, warning=FALSE}
library(caret)

# fit the model
fit <- knnreg(train[,4:6],train[,7],k=3)

# evaluate
pred2 <- predict(fit, test[,4:6])
cor_knn1 <- cor(pred2, test$Price)
mse_knn1 <- mean((pred2 - test$Price) ^2)
print(paste("cor=", cor_knn1))
```
```{r}
print(paste("mse=", mse_knn1))
```
As we can see, the results for kNN weren't quite as good as the results for the Linear Regression model.
(Cor for LinReg: ~0.77, kNN: ~0.68)
A reason for this difference might be that we didn't scale the data for kNN, which works better on scaled data.

### Scale the data for kNN

We are scaling both train and test data on the means and standard deviations of the training set.
This is so that information about the test data does not leak into the scaling.

```{r}
train_scaled <- train[4:6]
means <- sapply(train_scaled, mean)
stdvs <- sapply(train_scaled, sd)
train_scaled <- scale(train_scaled, center=means, scale=stdvs)
test_scaled <- scale(test[, 4:6], center=means, scale=stdvs)
```

### kNN on scaled data

```{r}
fit <- knnreg(train_scaled, train$Price, k=3)
pred3 <- predict(fit, test_scaled)
cor_knn2 <- cor(pred3, test$Price)
mse_knn2 <- mean((pred3 - test$Price) ^2)
print(paste("cor=", cor_knn2))
```
```{r}
print(paste("mse=", mse_knn2))
```
The kNN now has a *slightly* higher cor and lower mse than before scaling, but still not higher than the Linear Regression.
This might just be because we can only use 3 predictor values, because the other columns contain characters and not numeric values.
Compared to the first Linear Regression model we made, though, using only Duration and Total_Stops as predictors, we have a high increase in Correlation and decrease in mse.
(Cor for first lm model: 0.58, for kNN: 0.68)

### Find the best k
We will try various values of k and plot the results.
```{r}
cor_k <- rep(0, 20)
mse_k <- rep(0, 20)
i <- 1
for (k in seq(1, 39, 2)) {
  fit_k <- knnreg(train_scaled, train$Price, k=k)
  pred_k <- predict(fit_k, test_scaled)
  cor_k[i] <- cor(pred_k, test$Price)
  mse_k[i] <- mean((pred_k - test$Price)^2)
  print(paste("k=", k, cor_k[i], mse_k[i]))
  i <- i + 1
}
```
```{r}
plot(1:20, cor_k, lwd=2, col='red', ylab="", yaxt='n')
par(new=TRUE)
plot(1:20, mse_k, lwd=2, col='blue', labels=FALSE, ylab="", yaxt='n')
```

As we can see, the best value for k is at k = 5 (in this plot it is at 3, but that is because there is k=5 is at index 3 in lists cor_k and mse_k)

We can also check with min and max:
```{r}
which.min(mse_k)
```
```{r}
which.max(cor_k)
```
Since we have used k=3 (which was just a coincidence) in the above kNN regression already, we have our best data for this regression.

# Decision Tree for regression

```{r, warning = FALSE}
library(tree)
library(MASS)
tree1 <- tree(Price ~., data=train)
summary(tree1)
```

```{r, warning=FALSE}
pred4 <- predict(tree1, newdata=test)
cor_tree <- cor(pred4, test$Price)
print(paste("cor:", cor_tree))
```
```{r}
mse_tree <- mean((pred4 - test$Price) ^2)
print(paste("mse:", mse_tree))
```
So far, the correlation and mse are worse than for either kNN or Linear Regression.

```{r}
plot(tree1)
text(tree1, cex = 0.5, pretty = 0)
```


## Concusion

Comparing all the results, kNN and Linear Regression performed the best.
In the dataset there are only 3 predictors that are numeric values, which made it hard to fairly compare all 3 models.
The Linear Regression model got the best results by using all predictors. The kNN got the best results considering it could only use 3 predictors. And even then, it was only slightly worse than the Linear Regression model using all predictors. The Decision tree performed slightly worse than the kNN, as it was also only using 2 predictors. For this dataset specifically, the Linear Regression was the most powerful model. I think these results strongly differ with different types of data sets.